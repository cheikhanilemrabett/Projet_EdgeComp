import json
import numpy as np
from kafka import KafkaConsumer
import os

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙƒØ§ÙÙƒØ§
SUJET_KAFKA = 'fraud-model-updates'
SERVEUR_KAFKA = 'localhost:9092'
FICHIER_HISTORIQUE = 'historique_modele_global.json'

def moyennage_federe(tampon_modeles):
    """
    ØªØ·Ø¨ÙŠÙ‚ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© FedAvg:
    Global_Weight = Sum(Local_Weight * num_samples) / Total_Samples
    """
    total_echantillons = sum(m['num_samples'] for m in tampon_modeles)
    
    # Ø­Ø³Ø§Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ÙˆØ²ÙˆÙ†Ø©
    poids_ponderes = [np.array(m['weights']) * m['num_samples'] for m in tampon_modeles]
    biais_ponderes = [m['intercept'] * m['num_samples'] for m in tampon_modeles]
    
    # Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙˆØ§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠ
    poids_globaux = np.sum(poids_ponderes, axis=0) / total_echantillons
    biais_global = sum(biais_ponderes) / total_echantillons
    
    # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ù‚Ø© (Ù„Ù„Ø¹Ø±Ø¶ ÙÙ‚Ø·)
    precision_moyenne = sum(m['accuracy'] * m['num_samples'] for m in tampon_modeles) / total_echantillons
    
    return poids_globaux.tolist(), biais_global, precision_moyenne

def demarrer_serveur():
    print(f"ğŸ“¡ Le serveur central est en cours d'exÃ©cution... Ã‰coute sur le sujet : {SUJET_KAFKA}")
    
    consommateur = KafkaConsumer(
        SUJET_KAFKA,
        bootstrap_servers=SERVEUR_KAFKA,
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest'
    )

    tampon_modeles = []
    num_round = 1
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù ÙØ§Ø±Øº Ù„Ù„Ø³Ø¬Ù„
    with open(FICHIER_HISTORIQUE, 'w') as f:
        json.dump([], f)

    print("â³ En attente de mises Ã  jour des nÅ“uds (Agencies)...")
    
    for message in consommateur:
        donnees = message.value
        print(f"   ğŸ“¥ Mise Ã  jour reÃ§ue de : {donnees['node_id']} (PrÃ©cision locale : {donnees['accuracy']:.2%})")
        
        tampon_modeles.append(donnees)
        
        # Ù„Ù†ÙØªØ±Ø¶ Ø£Ù†Ù†Ø§ Ù†Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙƒÙ„Ù…Ø§ ÙˆØµÙ„Ù†Ø§ ØªØ­Ø¯ÙŠØ«ÙŠÙ† Ø£Ùˆ Ø£ÙƒØ«Ø±
        if len(tampon_modeles) >= 2:
            print(f"\nâš™ï¸  DÃ©but du processus de fusion (Round {num_round})...")
            
            # 1. Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯
            p_globaux, b_global, precision_g = moyennage_federe(tampon_modeles)
            
            print(f"   âœ… Nouveau modÃ¨le global gÃ©nÃ©rÃ© !")
            print(f"   ğŸ“Š PrÃ©cision globale agrÃ©gÃ©e : {precision_g:.2%}")
            
            # 2. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù Ù„Ø¹Ø±Ø¶Ù‡Ø§ ÙÙŠ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…
            enregistrement = {
                'round': num_round,
                'accuracy': precision_g,
                'participating_nodes': [m['node_id'] for m in tampon_modeles],
                'timestamp': message.timestamp
            }
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù‚Ø¯ÙŠÙ… ÙˆØªØ­Ø¯ÙŠØ«Ù‡
            try:
                with open(FICHIER_HISTORIQUE, 'r') as f:
                    historique = json.load(f)
            except:
                historique = []
                
            historique.append(enregistrement)
            
            with open(FICHIER_HISTORIQUE, 'w') as f:
                json.dump(historique, f)
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ù„Ù„Ø¬ÙˆÙ„Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©
            tampon_modeles = []
            num_round += 1
            print("------------------------------------------------")

if __name__ == "__main__":
    demarrer_serveur()