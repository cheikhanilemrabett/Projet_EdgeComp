from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
import json
import time
import traceback
from kafka import KafkaProducer

def main():
    try:
        print("ğŸš€ DÃ©marrage de Fog Node 2...")
        
        # 1. Ø¥Ù†Ø´Ø§Ø¡ Spark Session
        spark = SparkSession.builder \
            .appName("FogNode-2") \
            .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
            .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint_node2") \
            .getOrCreate()
        
        print("âœ… Spark Session crÃ©Ã©e")
        
        # 2. ØªØ¹Ø±ÙŠÙ Ù…Ø®Ø·Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        schema = StructType([
            StructField("node_id", StringType()),
            StructField("timestamp", StringType()),
            StructField("temperature", DoubleType()),
            StructField("vibration", DoubleType()),
            StructField("pressure", DoubleType()),
            StructField("anomaly", DoubleType())
        ])
        
        # 3. Kafka Producer
        print("ğŸ”Œ Tentative de connexion Ã  Kafka...")
        try:
            kafka_producer = KafkaProducer(
                bootstrap_servers=['localhost:9092'],
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )
            print("âœ… Connexion au Kafka Producer")
        except Exception as e:
            print(f"âŒ Ã‰chec de la connexion Ã  Kafka: {e}")
            spark.stop()
            return
        
        # 4. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Kafka
        print("ğŸ“– Lecture du topic sensor-data-node-2...")
        df = spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "sensor-data-node-2") \
            .option("startingOffsets", "earliest") \
            .option("failOnDataLoss", "false") \
            .load()
        
        # 5. ØªØ­ÙˆÙŠÙ„ JSON
        parsed_df = df.select(
            from_json(col("value").cast("string"), schema).alias("data")
        ).select("data.*")
        
        # 6. Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        def process_batch(batch_df, batch_id):
            try:
                count = batch_df.count()
                print(f"\n{'='*50}")
                print(f"ğŸ”§ Fog Node 2 - Batch {batch_id}")
                print(f"   Ã‰chantillons: {count}")
                
                if count > 0:
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
                    avg_temp = batch_df.selectExpr("avg(temperature)").first()[0]
                    avg_vibration = batch_df.selectExpr("avg(vibration)").first()[0]
                    
                    # Ø£ÙˆØ²Ø§Ù† Ù…Ø­Ø§ÙƒØ§Ø©
                    weights = [avg_temp, avg_vibration, 0.5]
                    
                    # Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
                    weights_msg = {
                        "node_id": "node-2",
                        "batch_id": int(batch_id),
                        "weights": weights,
                        "intercept": 1.0,
                        "num_samples": int(count),
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "type": "node_update"
                    }
                    
                    # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ model-weights
                    kafka_producer.send("model-weights", value=weights_msg)
                    kafka_producer.flush()
                    
                    print("ğŸ“¤ Poids envoyÃ©s â†’ model-weights")
                    print(f"   Poids: {[round(w, 2) for w in weights]}")
                else:
                    print("âš ï¸ Pas de donnÃ©es dans ce batch")
                
                print(f"{'='*50}")
                
            except Exception as e:
                print(f"âŒ Erreur dans process_batch: {e}")
                traceback.print_exc()
        
        # 7. Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        print("ğŸ¬ DÃ©marrage du Streaming Query...")
        
        query = parsed_df \
            .writeStream \
            .foreachBatch(process_batch) \
            .trigger(processingTime='10 seconds') \
            .option("checkpointLocation", "/tmp/checkpoint_fog_node_2") \
            .start()
        
        print("âœ… Fog Node 2 fonctionne et attend les donnÃ©es...")
        print("   Appuyez sur Ctrl+C pour arrÃªter\n")
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ Ø§Ù„Ø¥Ù†Ù‡Ø§Ø¡
        query.awaitTermination()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Fog Node 2 arrÃªtÃ© par l'utilisateur")
    except Exception as e:
        print(f"\nâŒ Erreur principale : {e}")
        traceback.print_exc()
    finally:
        print("\nğŸ§¹ Nettoyage des ressources...")
        try:
            spark.stop()
        except:
            pass

if __name__ == "__main__":
    main()
