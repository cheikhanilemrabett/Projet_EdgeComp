# cloud_aggregator.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, IntegerType
import json
import time
from kafka import KafkaProducer

# 1. Cr√©er la session Spark
print("‚òÅÔ∏è D√©marrage Cloud Aggregator...")
spark = SparkSession.builder \
    .appName("CloudAggregator") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint_cloud") \
    .getOrCreate()

# 2. D√©finir le sch√©ma des poids
schema_poids = StructType([
    StructField("node_id", StringType()),
    StructField("batch_id", IntegerType()),
    StructField("weights", ArrayType(DoubleType())),
    StructField("intercept", DoubleType()),
    StructField("num_samples", IntegerType()),
    StructField("timestamp", StringType()),
    StructField("type", StringType())
])

# 3. Producer Kafka
kafka_producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# 4. Lire les poids de Kafka
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "model-weights") \
    .option("startingOffsets", "earliest") \
    .load()

# 5. Convertir JSON
parsed_df = df.select(
    from_json(col("value").cast("string"), schema_poids).alias("data")
).select("data.*")

# 6. Fonction Federated Averaging
def aggreg_federree(batch_df, batch_id):
    lignes = batch_df.collect()
    
    if len(lignes) > 0:
        print(f"\n{'='*60}")
        print(f"‚òÅÔ∏è Cloud Aggregator - Batch {batch_id}")
        print(f"   N≈ìuds: {len(lignes)}")
        
        # Afficher chaque n≈ìud
        for i, ligne in enumerate(lignes):
            print(f"   N≈ìud {ligne['node_id']}: {ligne['num_samples']} √©chantillons")
        
        # Calculer total √©chantillons
        total_echantillons = sum([ligne['num_samples'] for ligne in lignes])
        print(f"   Total √©chantillons: {total_echantillons}")
        
        if total_echantillons > 0:
            # Appliquer FedAvg
            nb_features = len(lignes[0]['weights'])
            poids_moyens = [0.0] * nb_features
            intercept_moyen = 0.0
            
            for ligne in lignes:
                facteur = ligne['num_samples'] / total_echantillons
                
                # Poids pond√©r√©s
                for i in range(nb_features):
                    poids_moyens[i] += ligne['weights'][i] * facteur
                
                # Intercept pond√©r√©
                intercept_moyen += ligne['intercept'] * facteur
            
            # Cr√©er mod√®le global
            modele_global = {
                "model_id": f"global_model_{batch_id}",
                "weights": poids_moyens,
                "intercept": float(intercept_moyen),
                "total_samples": int(total_echantillons),
                "num_nodes": len(lignes),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "type": "global_model"  # Important pour le dashboard
            }
            
            print(f"   ‚úÖ Mod√®le global cr√©√©:")
            print(f"      - Poids moyens: {[round(p, 4) for p in poids_moyens]}")
            print(f"      - Intercept: {round(intercept_moyen, 4)}")
            
            # Envoyer au topic global-model
            kafka_producer.send("global-model", value=modele_global)
            kafka_producer.flush()
            print(f"   üì§ Envoy√© ‚Üí global-model")
        
        print(f"{'='*60}")

# 7. D√©marrer l'agr√©gation
query = parsed_df \
    .writeStream \
    .foreachBatch(aggreg_federree) \
    .trigger(processingTime='15 seconds') \
    .option("checkpointLocation", "/tmp/checkpoint_cloud_agg") \
    .start()

print("‚úÖ Cloud Aggregator pr√™t - En attente de poids...")
query.awaitTermination()