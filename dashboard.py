import streamlit as st
import json
import pandas as pd
from kafka import KafkaConsumer
import matplotlib.pyplot as plt
import time

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(
    page_title="IoT Federated Learning Monitor",
    layout="wide",
    page_icon="â˜ï¸"
)

st.title("â˜ï¸ Cloud Aggregator & Live Monitoring")
st.markdown("---")

# --- ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª (Session State) ---
# Ù†Ø­ØªØ§Ø¬ Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ÙƒÙŠ Ù„Ø§ ØªØ®ØªÙÙŠ Ø¹Ù†Ø¯ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø©
if 'data_history' not in st.session_state:
    st.session_state['data_history'] = []

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Kafka Consumer ---
# Ù†Ø³ØªØ®Ø¯Ù… @st.cache_resource Ù„ÙƒÙŠ Ù„Ø§ ÙŠØ¹ÙŠØ¯ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Kafka Ù…Ø¹ ÙƒÙ„ ØªØ­Ø¯ÙŠØ« Ù„Ù„ØµÙØ­Ø©
@st.cache_resource
def init_consumer():
    return KafkaConsumer(
        'model-weights',
        bootstrap_servers=['localhost:9092'],
        auto_offset_reset='earliest',  # Ø§Ù‚Ø±Ø£ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø£ÙˆÙ„Ø§Ù‹
        enable_auto_commit=True,
        group_id='dashboard-group-v2', # ØªØºÙŠÙŠØ± Ø§Ù„Ø¬Ø±ÙˆØ¨ Ù„Ø¶Ù…Ø§Ù† Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯
        value_deserializer=lambda x: json.loads(x.decode('utf-8'))
    )

consumer = init_consumer()

# --- ØªØ®Ø·ÙŠØ· Ø§Ù„ØµÙØ­Ø© (Layout) ---
# Ù†Ù†Ø´Ø¦ Ø£Ù…Ø§ÙƒÙ† ÙØ§Ø±ØºØ© (Placeholders) Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("ğŸ“Š Global Model Metrics")
    metrics_placeholder = st.empty()
    logs_placeholder = st.empty()

with col2:
    st.subheader("ğŸ“ˆ Weights Convergence")
    chart_placeholder = st.empty()

# --- Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
st.toast("Listening for Spark updates...", icon="ğŸ“¡")

# Ø²Ø± Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙŠØ¯ÙˆÙŠØ§Ù‹
stop_button = st.button("Stop Monitoring")

while not stop_button:
    # 1. Ù…Ø­Ø§ÙˆÙ„Ø© Ø³Ø­Ø¨ Ø±Ø³Ø§Ø¦Ù„ Ø¬Ø¯ÙŠØ¯Ø© (Ù„Ù…Ø¯Ø© 0.5 Ø«Ø§Ù†ÙŠØ© ÙÙ‚Ø·)
    # Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³Ø±: poll Ù„Ø§ ØªØ¬Ù…Ø¯ Ø§Ù„Ø´Ø§Ø´Ø© Ù„Ù„Ø£Ø¨Ø¯ Ù…Ø«Ù„ for loop
    msg_pack = consumer.poll(timeout_ms=500)

    # 2. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø±Ø³Ø§Ø¦Ù„ØŒ Ù†Ø¶ÙŠÙÙ‡Ø§ Ù„Ù„Ù‚Ø§Ø¦Ù…Ø©
    if msg_pack:
        for tp, messages in msg_pack.items():
            for message in messages:
                data = message.value
                st.session_state['data_history'].append(data)
    
    # 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¹Ø±Ø¶Ù‡Ø§ (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ØºÙŠØ± ÙØ§Ø±ØºØ©)
    if len(st.session_state['data_history']) > 0:
        df = pd.DataFrame(st.session_state['data_history'])
        
        # --- Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª (Federated Averaging) ---
        # Ù†Ø£Ø®Ø° Ø¢Ø®Ø± 20 ØªØ­Ø¯ÙŠØ«Ø§Ù‹ Ù„Ù†ÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
        recent_df = df.tail(20)
        global_coef = recent_df['coef'].mean()
        global_intercept = recent_df['intercept'].mean()
        total_updates = len(df)
        last_node = df.iloc[-1]['node_id']

        # --- ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø±Ù‚Ø§Ù… (Metrics) ---
        with metrics_placeholder.container():
            kpi1, kpi2 = st.columns(2)
            kpi1.metric("Global Slope (Weights)", f"{global_coef:.4f}")
            kpi2.metric("Global Bias (Intercept)", f"{global_intercept:.4f}")
            st.info(f"Last update from: **{last_node}** | Total Packets: {total_updates}")
            st.success(f"Final Model Equation:\n\n $y = {global_coef:.2f}x + {global_intercept:.2f}$")

        # --- ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ ---
        with chart_placeholder.container():
            fig, ax = plt.subplots(figsize=(8, 4))
            
            # Ø±Ø³Ù… Ù†Ù‚Ø§Ø· ÙƒÙ„ Ø¹Ù‚Ø¯Ø© Ø¨Ù„ÙˆÙ† Ù…Ø®ØªÙ„Ù
            groups = recent_df.groupby('node_id')
            for name, group in groups:
                ax.plot(group.index, group['coef'], marker='o', linestyle='', label=name, alpha=0.6)
            
            # Ø±Ø³Ù… Ø§Ù„Ø®Ø· Ø§Ù„Ù…ØªÙˆØ³Ø· (Global Model)
            ax.axhline(y=global_coef, color='red', linestyle='--', linewidth=2, label='Global Model')
            
            ax.set_title("Live Weight Updates (FedAvg)")
            ax.set_ylabel("Coefficient Value")
            ax.set_xlabel("Update Sequence")
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            st.pyplot(fig)
            plt.close(fig) # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©

        # --- Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„Ø§Øª ---
        with logs_placeholder.container():
            st.write("Recent Raw Data:")
            st.dataframe(recent_df[['node_id', 'batch_id', 'coef', 'intercept']].tail(5), hide_index=True)

    else:
        # Ø´Ø§Ø´Ø© Ø§Ù†ØªØ¸Ø§Ø± Ø¥Ø°Ø§ Ù„Ù… ØªØµÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯
        with metrics_placeholder.container():
            st.warning("Waiting for data from Kafka...")

    # 4. ØªÙˆÙ‚Ù Ù„Ø­Ø¸ÙŠ Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø­Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
    time.sleep(1) 
    # Ù„Ø§ Ù†Ø­ØªØ§Ø¬ st.rerun Ù‡Ù†Ø§ Ù„Ø£Ù†Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… while loop ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ Placeholders